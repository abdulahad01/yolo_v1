{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo71-n4BfYX9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "!unzip /content/gdrive/MyDrive/Dl/archive.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDsP1yIH9AQu"
      },
      "outputs": [],
      "source": [
        "# Imports \n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as FT\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORYLhV_P-JQ3"
      },
      "outputs": [],
      "source": [
        "cfg = [\n",
        "    # (Filters, size, stride, padding)\n",
        "    (64,7,2,3),\n",
        "    'm',\n",
        "    (192,3,1,1),\n",
        "    'm',\n",
        "    (128,1,1,0),\n",
        "    (256,3,1,1),\n",
        "    (256,1,1,0),\n",
        "    (512,3,1,1),\n",
        "    'm',\n",
        "    [4, (256,1,1,0), (512,3,1,1)],\n",
        "    (512,1,1,0),\n",
        "    (1024,3,1,1),\n",
        "    'm',\n",
        "    [2,(512,1,1,0),(1024,3,1,1)],\n",
        "    (1024,3,1,1),\n",
        "    (1024,3,2,1),\n",
        "    (1024,3,1,1),\n",
        "    (1024,3,1,1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qkhk3MrLIK9R"
      },
      "outputs": [],
      "source": [
        "# Creating each CNN block\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel, stride, padding):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = kernel, stride = stride, padding = padding, bias=False)\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "  def forward(self, x):\n",
        "    return self.leaky_relu(self.bn(self.conv(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7jyBJtE40Od"
      },
      "outputs": [],
      "source": [
        "class YOLO_V1(nn.Module):\n",
        "  def __init__(self, in_channels = 3, **kwargs):\n",
        "    super().__init__()\n",
        "    self._in_channels = in_channels\n",
        "    self._config = cfg\n",
        "    self._darknet = self._create_darknet(self._config)\n",
        "    self._fcs = self._create_fcs(**kwargs)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self._darknet(x)\n",
        "    x = self._fcs(x)\n",
        "    return x\n",
        "\n",
        "  # Darknet is the name of the CNN architecture\n",
        "  def _create_darknet(self,config):\n",
        "    _layers = []\n",
        "    in_channels = self._in_channels\n",
        "    for item in config:\n",
        "      if type(item) == tuple:\n",
        "        _layers += [\n",
        "            CNNBlock(in_channels, item[0],item[1], item[2], item[3])\n",
        "        ]\n",
        "        in_channels = item[0]\n",
        "      elif type(item) == str:\n",
        "        _layers += [nn.MaxPool2d(kernel_size=2,stride=2)]\n",
        "      elif type(item) == list:\n",
        "        conv_1 = item[1]\n",
        "        conv_2 = item[2]\n",
        "        for i in range(item[0]):\n",
        "          _layers += [CNNBlock(in_channels, conv_1[0], conv_1[1], conv_1[2], conv_1[3])]\n",
        "          _layers += [CNNBlock(conv_1[0], conv_2[0], conv_2[1], conv_2[2], conv_2[3])]\n",
        "    return nn.Sequential(*_layers)\n",
        "  def _create_fcs(self, split_size, bb_no, no_classes):\n",
        "    self.split_size = split_size\n",
        "    self.bb_no = bb_no\n",
        "    self.no_classes = no_classes\n",
        "    return nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(self.split_size*split_size*1024, 4096),\n",
        "        nn.Linear(4096, self.split_size*split_size*(self.bb_no*5 + self.no_classes))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hr3korPeej8",
        "outputId": "6fd5bb81-201b-4cdd-fc92-04bccb104e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1470])\n"
          ]
        }
      ],
      "source": [
        "def test(S = 7, num_boxes = 2, no_classes = 20 ):\n",
        "  model = YOLO_V1(split_size = S, bb_no = num_boxes, no_classes=no_classes)\n",
        "  x = torch.rand(1,3,448,448)\n",
        "  print(model(x).shape)\n",
        "\n",
        "test()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utility functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analytic functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6juFNU2zPEJj"
      },
      "outputs": [],
      "source": [
        "def intersection_over_union(bb_1, bb_2):\n",
        "  \"\"\"\n",
        "  Get two bounding boxes and calculate their IOU\n",
        "  i/p format : (batch_Size, S, S, box_cordinates[5])\n",
        "  box_cordinates : (c_x,c_y,w,h) \n",
        "  \"\"\"\n",
        "  bb_1_x_left = (bb_1[...,0:1] - bb_1[...,2:3])/2\n",
        "  bb_1_y_top = (bb_1[...,1:2] - bb_1[...,3:4])/2\n",
        "  bb_1_x_right = (bb_1[...,0:1] + bb_1[...,2:3])/2\n",
        "  bb_1_y_bottom = (bb_1[...,1:2] + bb_1[...,3:4])/2\n",
        "  # print(bb_1_x_left, bb_1_x_right)\n",
        "  # print(bb_1_y_top, bb_1_y_bottom)\n",
        "  \n",
        "  bb_2_x_left = (bb_2[...,0:1] - bb_2[...,2:3])/2\n",
        "  bb_2_y_top =  (bb_2[...,1:2] - bb_2[...,3:4])/2\n",
        "  bb_2_x_right = (bb_2[...,0:1] + bb_2[...,2:3])/2\n",
        "  bb_2_y_bottom = (bb_2[...,1:2] + bb_2[...,3:4])/2\n",
        "\n",
        "  x_left = torch.max(bb_1_x_left, bb_2_x_left)\n",
        "  y_top = torch.max(bb_1_y_top, bb_2_y_top)\n",
        "\n",
        "  # print(x_left, y_top)\n",
        "\n",
        "  x_right = torch.min(bb_1_x_right, bb_2_x_right)\n",
        "  y_bottom = torch.min(bb_1_y_bottom, bb_2_y_bottom)\n",
        "\n",
        "  # print(x_right, y_bottom)\n",
        "\n",
        "  inter_width = x_right - x_left\n",
        "  inter_height = y_bottom - y_top\n",
        "\n",
        "  # print(inter_width, inter_height)\n",
        "\n",
        "  inter_area = inter_width * inter_height\n",
        "\n",
        "  total_area = (bb_1[...,2:3]* bb_1[...,3:4]) + (bb_2[...,2:3]* bb_2[...,3:4])\n",
        "\n",
        "  union = total_area- inter_area\n",
        "\n",
        "  iou = inter_area/ union\n",
        "\n",
        "  return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ybiIU8zA7b",
        "outputId": "6c71f6a7-f590-4cb6-99e4-98527615d03c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intersection_over_union(torch.tensor([0,0,2,3]), torch.tensor([0,0,2,3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_average_precision(preds, labels, iou_threshold= 0.5, NUM_CLASSES= 20):\n",
        "    '''\n",
        "    Calculates the mean average precision scores\n",
        "    pred =[[idx,class,obj,x,y,w,h],...]\n",
        "    label =[[idx,class,oj,x,y,w,h],...]\n",
        "    '''\n",
        "    avg_prec = []\n",
        "    # For each class \n",
        "    for c in range(NUM_CLASSES):\n",
        "        # variables to store the predictions and true labels for the class\n",
        "        predictions = []\n",
        "        true_label = []\n",
        "        # print(predictions, true_label)\n",
        "\n",
        "        # Filter out predicitons and true lables for the class\n",
        "        predictions = [pred for pred in preds if pred[1] == c]\n",
        "        true_label = [label for label in labels if label[1] == c]\n",
        "\n",
        "        if len(true_label) == 0:\n",
        "            continue\n",
        "\n",
        "        # Number of ground truths per image\n",
        "        boxes_per_image = Counter([bbox[0] for bbox in true_label])\n",
        "        # print(boxes_per_image)\n",
        "        for key, val in boxes_per_image.items():\n",
        "            boxes_per_image[key] = torch.zeros(val)\n",
        "        \n",
        "\n",
        "        # variables for calucating precision and recall\n",
        "        true_pos = torch.tensor([0]*len(predictions))\n",
        "        false_pos = torch.tensor([0]*len(predictions))\n",
        "\n",
        "        # Sort in order of highest objectness score \n",
        "        # predictions.sort(key=lambda x:x[2], reverse= True)\n",
        "        for id_pred, pred in enumerate(predictions):\n",
        "            image_boxes = [bbox for bbox in true_label if bbox[0] == pred[0]]\n",
        "            best_iou = 0\n",
        "            for gt_idx, gt in enumerate(image_boxes):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(pred[3:]),\n",
        "                    torch.tensor(gt[3:])\n",
        "                    )\n",
        "                # print(iou)\n",
        "                if best_iou < iou:\n",
        "                    best_iou = iou \n",
        "                    best_idx = id_pred\n",
        "                    true_box = gt[0]\n",
        "                    true_box_id = gt_idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                if boxes_per_image[true_box][true_box_id] == 0:\n",
        "                    true_pos[best_idx] = 1\n",
        "                    boxes_per_image[true_box][true_box_id] = 1\n",
        "                else:\n",
        "                    false_pos[id_pred] = 1\n",
        "            else:\n",
        "                false_pos[id_pred] = 1\n",
        "        \n",
        "        tp_cumsum = torch.cumsum(true_pos, dim = 0)\n",
        "        fp_cumsum = torch.cumsum(false_pos, dim = 0)\n",
        "\n",
        "        # print(true_pos, false_pos)\n",
        "        # print(tp_cumsum, fp_cumsum)\n",
        "\n",
        "        precision = tp_cumsum/(tp_cumsum+fp_cumsum + 1e-6)\n",
        "        recall = tp_cumsum/(len(true_label)+1e-6)\n",
        "\n",
        "        precision = torch.cat((torch.tensor([1]), precision))\n",
        "        recall = torch.cat((torch.tensor([0]), recall))\n",
        "\n",
        "        # print(precision, recall)\n",
        "        # Using trapezoidal sum approximation\n",
        "        avg_prec.append(torch.trapz(precision,recall))\n",
        "        # print(avg_prec)\n",
        "\n",
        "    \n",
        "    return sum(avg_prec)/len(avg_prec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def non_max_suppression(predictions, iou_threshold = 0.5):\n",
        "    '''\n",
        "    Algorithm to suppress multiple boxes for the same detection\n",
        "    '''\n",
        "    best_boxes = []\n",
        "    predictions = sorted(predictions, key = lambda x:x[1], reverse=True)\n",
        "    print(predictions)\n",
        "    assert type(predictions) == list\n",
        "\n",
        "    while(predictions):\n",
        "        best_box = predictions.pop(0)\n",
        "        predictions = [box for box in predictions if intersection_over_union(torch.tensor(box[2:]) , torch.tensor(best_box[2:])) < iou_threshold or best_box[0] != box[0]]\n",
        "\n",
        "        best_boxes.append(best_box)\n",
        "    \n",
        "    return best_boxes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convenience Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cell_to_image(pred_boxes, S=7):\n",
        "    '''\n",
        "    Converts the predictions from cell cordinates to Image cordinates\n",
        "    '''\n",
        "\n",
        "    batch_size = pred_boxes.shape[0]\n",
        "    pred_boxes = pred_boxes.reshape(-1, S, S, 30)\n",
        "    boxes_1 = pred_boxes[...,21:25]\n",
        "    boxes_2 = pred_boxes[..., 26:30]\n",
        "    scores_1 = pred_boxes[...,20:21]\n",
        "    scores_2 = pred_boxes[...,25:26]\n",
        "    best_confidence = torch.max(scores_1, scores_2)\n",
        "    best_score = torch.cat((scores_1, scores_2), dim = -1).argmax(-1).unsqueeze(-1)\n",
        "    # print(best_score.shape, boxes_1.shape)\n",
        "    # print(((best_score) * boxes_2).shape)\n",
        "    best_class = pred_boxes[...,:20].argmax(-1).unsqueeze(-1)\n",
        "    # print(best_class.shape)\n",
        "\n",
        "    best_box = (1-best_score)*boxes_1 + (best_score * boxes_2)\n",
        "    # print(best_box.shape)\n",
        "    for batch in range(batch_size):\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                x = (best_box[...,:1]+j)*1/S\n",
        "                y = (best_box[...,1:2]+i)*1/S\n",
        "                w_h = (best_box[...,2:4])*1/S\n",
        "                bb_concat = torch.cat((x,y,w_h),-1)\n",
        "    corrected_box = torch.cat((best_class, best_confidence, bb_concat), dim = -1)\n",
        "    return corrected_box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    threshold,\n",
        "    pred_format=\"cells\",\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        true_bboxes = cellboxes_to_boxes(labels)\n",
        "        bboxes = cellboxes_to_boxes(predictions)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                # many will get converted to 0 pred\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "    converted_pred = (out).reshape(out.shape[0], S * S, -1)\n",
        "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
        "    all_bboxes = []\n",
        "\n",
        "    for ex_idx in range(out.shape[0]):\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox_idx in range(S * S):\n",
        "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
        "        all_bboxes.append(bboxes)\n",
        "\n",
        "    return all_bboxes\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_image(image, boxes):\n",
        "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    # box[0] is x midpoint, box[2] is width\n",
        "    # box[1] is y midpoint, box[3] is height\n",
        "\n",
        "    # Create a Rectangle potch\n",
        "    for box in boxes:\n",
        "        box = box[2:]\n",
        "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"r\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLO Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWqSJcNjzLO6"
      },
      "outputs": [],
      "source": [
        "class YOLO_LOSS(nn.Module):\n",
        "  \"\"\"\n",
        "  The yolo loss function :\n",
        "  Hyperparameter : Lambda coord, lambda no obj\n",
        "  1. A mean squared loss over box cordinates ( x,y,w,h). Takes loss over the best boxes (IOU scores) and ignores boxes without objects\n",
        "  2. A mean squared loss over objectness score\n",
        "  3. A mean squared loss over no object (inverse of objectness)\n",
        "  4. A mean squared loss over the class probabilities\n",
        "  \"\"\"\n",
        "  def __init__(self, S=7, B=2, C=20):\n",
        "    super().__init__()\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.mse = nn.MSELoss(reduction='sum')\n",
        "    self.lambda_noobj = 0.5\n",
        "    self.lambda_coord = 5\n",
        "  \n",
        "  def forward(self, predictions, target):\n",
        "    predictions = predictions.reshape(-1, self.S, self.S, self.C+ self.B*5)\n",
        "    # 0:20 : Class scores \n",
        "    # 20 : Probability scores\n",
        "    # 21:25 : Box 1\n",
        "    # 25 : Probability scores\n",
        "    # 26:30 : Box 2\n",
        "    iou_b1 = intersection_over_union(predictions[..., 21:25], target[...,21:25])\n",
        "    iou_b2 = intersection_over_union(predictions[...,26:30], target[...,21:25])\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim = 0)\n",
        "    iou_maxes, best_box = torch.max(ious, dim=0)  # best box = 0 or 1\n",
        "    # Target shape :  (batch, num_boxes, 25--> obj, x,y,w,h,20 classes)\n",
        "    exists_box = target[..., 20].unsqueeze(3) #Iobj_i , 3 Dim tensor\n",
        "\n",
        "    ###                           ###\n",
        "    # Box cordinates : ( x, y, w, h)#\n",
        "    ###                           ###\n",
        "    # 4 Dim -> 3 Dim\n",
        "    # print(exists_box.shape, best_box.shape, predictions.shape)\n",
        "    box_predictions = exists_box * ( best_box * predictions[...,26:30] +\n",
        "                                    (1 - best_box) * predictions[..., 21:25])\n",
        "    box_targets = exists_box * target[..., 21:25]\n",
        "    # for width and height\n",
        "    box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * \\\n",
        "                                            torch.sqrt( torch.abs(box_predictions[...,2:4] + 1e-6))\n",
        "    box_targets[...,2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "    box_loss = self.mse(\n",
        "        torch.flatten(box_predictions, end_dim=-2),\n",
        "        torch.flatten(box_targets, end_dim = -2)\n",
        "    )\n",
        "\n",
        "    ###              ##\n",
        "    # For object loss #\n",
        "    ###              ##\n",
        "    pred_box = (\n",
        "        best_box * predictions[...,25:26] + ( 1- best_box) * predictions[..., 20:21]\n",
        "    )\n",
        "\n",
        "    object_loss = self.mse(\n",
        "        torch.flatten(exists_box * pred_box),\n",
        "        torch.flatten(exists_box * target[..., 20:21])\n",
        "    )\n",
        "\n",
        "    # For no object\n",
        "    no_object_loss = self.mse(\n",
        "        torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim =1),\n",
        "        torch.flatten((1 - exists_box) * target[..., 20:21], start_dim =1)\n",
        "    )\n",
        "    no_object_loss += self.mse(\n",
        "        torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim =1),\n",
        "        torch.flatten((1 - exists_box) * target[..., 20:21], start_dim =1)\n",
        "    )\n",
        "\n",
        "    ###\n",
        "    # Class loss\n",
        "    ###\n",
        "\n",
        "    class_loss = self.mse(\n",
        "        torch.flatten( exists_box * predictions[..., :20], end_dim = -2),\n",
        "        torch.flatten( exists_box * target[..., :20], end_dim = -2)\n",
        "    )\n",
        "\n",
        "    # From paper \n",
        "    loss = (\n",
        "        self.lambda_coord * box_loss +\n",
        "        object_loss +\n",
        "        self.lambda_noobj * no_object_loss\n",
        "        +class_loss\n",
        "    )\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfME93T9M8ow"
      },
      "outputs": [],
      "source": [
        "class YOLO_VOC(torch.utils.data.Dataset):\n",
        "  def __init__(self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform= None):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.label_dir = label_dir\n",
        "    self.transform = transform\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.annotations))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
        "    boxes = []\n",
        "    with open((label_path)) as f:\n",
        "      for label in f.readlines():\n",
        "        class_label, x,y, w, h = [\n",
        "            float(x) if float(x) != int(float(x)) else int(x) \\\n",
        "            for x in label.replace(\"\\n\",\"\").split()\n",
        "        ]\n",
        "        boxes.append([class_label, x, y, w, h])\n",
        "    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "    image = Image.open(img_path)\n",
        "    boxes = torch.tensor(boxes)\n",
        "    # print(self.transform)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    label_matrix = torch.zeros((self.S, self.S, self.C + 5*self.B))\n",
        "    for box in boxes:\n",
        "      class_label, x, y, w, h = box.tolist()\n",
        "      class_label  = int(class_label)\n",
        "      i, j = int(self.S * y), int(self.S * x)\n",
        "      x_cell, y_cell = self.S*x - j, self.S * y -i\n",
        "      width_cell, height_cell =(\n",
        "          w * self.S,\n",
        "          h * self.S,\n",
        "      )\n",
        "      if label_matrix[i, j , 20] == 0:\n",
        "        label_matrix[i,j, 20] = 1\n",
        "        box_coordinates = torch.tensor(\n",
        "            [x_cell, y_cell, width_cell, height_cell]\n",
        "        )\n",
        "        label_matrix[i, j , 21:25] = box_coordinates\n",
        "        label_matrix[i,j, class_label] = 1\n",
        "    \n",
        "    return image, label_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAdHJDvApapH"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_dir = \"/content/images\"\n",
        "label_dir = \"/content/labels\"\n",
        "csv_file = \"/content/100examples.csv\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.Resize((448,448)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset = YOLO_VOC(csv_file, img_dir, label_dir, transform = transform),\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s0lrIBJI-kP"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, optimizer, loss_fn):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    mean_loss = []\n",
        "    model.train()\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        mean_loss.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "\n",
        "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrEO0SRYLFAP"
      },
      "outputs": [],
      "source": [
        "model = YOLO_V1(split_size = 7, bb_no =2, no_classes = 20).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 2e-6)\n",
        "loss_fn = YOLO_LOSS()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIYMctB0abig",
        "outputId": "c9a217fb-208d-4a14-ec0b-b8bf64e3f93b"
      },
      "outputs": [],
      "source": [
        "LOAD_MODEL = False\n",
        "if LOAD_MODEL:\n",
        "      load_checkpoint(torch.load(\"/content/output.pth.tar\"), model, optimizer)\n",
        "EPOCHS = 1000\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "        pred_boxes, target_boxes = get_bboxes(\n",
        "            train_loader, model, iou_threshold=0.5, threshold=0.4\n",
        "        )\n",
        "\n",
        "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5)\n",
        "        print(f\"Train mAP: {mean_avg_prec}\")\n",
        "\n",
        "        if mean_avg_prec > 0.9:\n",
        "           checkpoint = {\n",
        "               \"state_dict\": model.state_dict(),\n",
        "               \"optimizer\": optimizer.state_dict(),\n",
        "           }\n",
        "           save_checkpoint(checkpoint, filename=\"/content/output.pth.tar\")\n",
        "           import time\n",
        "           time.sleep(10)\n",
        "\n",
        "        train(train_loader, model, optimizer, loss_fn)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
